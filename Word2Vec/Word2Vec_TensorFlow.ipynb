{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Word2Vec in TensorFlow Basic\n",
    "\n",
    "This is a basic version of implementation. It does NOT scale on big dataset.\n",
    "\n",
    "- Original tutorial link: https://www.tensorflow.org/get_started/os_setup#anaconda_installation\n",
    "- Installation of TensorFlow: https://www.tensorflow.org/get_started/os_setup#anaconda_installation\n",
    "\n",
    "- Word2Vec has two flavors, Skip-gram and CBOW. We use Skip-gram in this tutorial.\n",
    "\n",
    "## Skip-gram vs CBOW\n",
    "\n",
    "1. Skip-gram models the conditional probability as $P(\\text{context_word}\\quad | \\quad \\text{target_word})$, while CBOW models the conditional probability in an inverse way, i.e. $P(\\text{target_word} \\quad | \\quad \\text{context})$.\n",
    "\n",
    "## Data set\n",
    "\n",
    "http://mattmahoney.net/dc/text8.zip\n",
    "\n",
    "After unzip, it is a single text file of size ~95M, it is a big one-line text document. The following is the first 1000 bytes in the file (print whole takes a long time):\n",
    "\n",
    "     anarchism originated as a term of abuse first used against early working class radicals including the diggers of the english revolution and the sans culottes of the french revolution whilst the term is still used in a pejorative way to describe any act that used violent means to destroy the organization of society it has also been taken up as a positive label by self defined anarchists the word anarchism is derived from the greek without archons ruler chief king anarchism as a political philosophy is the belief that rulers are unnecessary and should be abolished although there are differing interpretations of what this means anarchism also refers to related social movements that advocate the elimination of authoritarian institutions particularly the state the word anarchy as most anarchists use it does not imply chaos nihilism or anomie but rather a harmonious anti authoritarian society in place of what are regarded as authoritarian political structures and coercive economic instituti\n",
    "\n",
    "The data seems to be pre-processed already, e.g. punctuation removed, convert to lower cases.\n",
    "\n",
    "## Data Preparation\n",
    "\n",
    "The raw data is usually a corpus (i.e. a collection of articles/reviews/...). There are a few data preprosessing steps before you can learn the embeddings.\n",
    "\n",
    "- Cleaning data to remove any noisy/non-informative elements, e.g. stop words maybe. \n",
    "- Integerizing the cleaned text corpus with a indexed vocabulary. That is, each word in the vocabulary is assgined a unique integer as its index (e.g. 1-> father, 2-> mother, 3 - loves, ... ), then replace each word in the corpus with the corresponding index. (e.g. if the corpus has the sentence \"father loves mother\", then after integerization, it becomes \"1 3 2\".) Details can be found in https://www.tensorflow.org/code/tensorflow/examples/tutorials/word2vec/word2vec_basic.py.\n",
    "- Some words are very rare in the corpus, we might not want to model them. So one way to handle rare words is to replace them with a common token, e.g. UNKOWN. Then the embedding is learned for UNKOWN.\n",
    "\n",
    "## Input Format for Skip-gram Algorithm\n",
    "\n",
    "- A batch full of integers representing the source context words.\n",
    "- Target words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import collections\n",
    "import numpy as np\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We already unzipped the data as a single text file.\n",
    "# tf.compat.as_str converts input into a string\n",
    "with open('text8', 'r') as f:\n",
    "    words = tf.compat.as_str(f.read()).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against']\n"
     ]
    }
   ],
   "source": [
    "# Print a few words in the dataset\n",
    "print words[:10]\n",
    "\n",
    "# So data is a list of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Pre-process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_size = 50000  # only keep top 50,000 words (in terms of frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count = [['UNKOWN', -1]]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# keep top 49,999 words from the original dataset, \n",
    "# leave 1 for UNKNOWN.\n",
    "count.extend(collections.Counter(words).most_common(vocab_size - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['UNKOWN', -1],\n",
       " ('the', 1061396),\n",
       " ('of', 593677),\n",
       " ('and', 416629),\n",
       " ('one', 411764),\n",
       " ('in', 372201),\n",
       " ('a', 325873),\n",
       " ('to', 316376),\n",
       " ('zero', 264975),\n",
       " ('nine', 250430)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2idx = dict()\n",
    "for word, _ in count:\n",
    "    w2idx[word] = len(w2idx)  # assign a unqiue index (1,2,...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "unkown_count = 0\n",
    "\n",
    "# Integerize text document (replace word with index)\n",
    "for word in words:\n",
    "    if word in w2idx:\n",
    "        index = w2idx[word]\n",
    "    else:\n",
    "        index = 0\n",
    "        unkown_count += 1\n",
    "    data.append(index)\n",
    "\n",
    "count[0][1] = unkown_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx2w = dict( zip(w2idx.values(), w2idx.keys()) )  # reverse key-value (make key as value, and value as key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del words  # no need anymore (save memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Most common words (+UNK)', [['UNKOWN', 418391], ('the', 1061396), ('of', 593677), ('and', 416629), ('one', 411764)])\n"
     ]
    }
   ],
   "source": [
    "print('Most common words (+UNK)', count[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Sample data', [5239, 3084, 12, 6, 195, 2, 3137, 46, 59, 156], ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against'])\n"
     ]
    }
   ],
   "source": [
    "print('Sample data', data[:10], [idx2w[i] for i in data[:10]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preapre a training batch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Preapre a training batch\n",
    "\n",
    "data_index = 0 \n",
    "\n",
    "def generate_batch(batch_size, num_skips, skip_window):\n",
    "    global data_index\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    span = 2*skip_window + 1  #  total number of words in a context, i.e. [skip_window, target, skip_window]\n",
    "    buffer = collections.deque(maxlen=span)  # list-like container with fast appends and pops on either end\n",
    "    for _ in range(span):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)  # go through from first word to the last in the data\n",
    "    for i in range(batch_size//num_skips):\n",
    "        target = skip_window  # middle word in the span\n",
    "        targets_to_avoid = [skip_window]\n",
    "        for j in range(num_skips):\n",
    "            while target in targets_to_avoid:\n",
    "                target = random.randint(0, span-1)  # why random sample?\n",
    "            targets_to_avoid.append(target)\n",
    "            batch[i*num_skips + j] = buffer[skip_window]  # middle word is always the predictor\n",
    "            labels[i*num_skips + j, 0] = buffer[target]\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    # Backtrack a little bit to avoid skipping words in the end of a batch\n",
    "    data_index = (data_index + len(data) - span) % len(data)\n",
    "    return batch, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3137, 'abuse', '->', 195, 'term')\n",
      "(3137, 'abuse', '->', 2, 'of')\n",
      "(46, 'first', '->', 3137, 'abuse')\n",
      "(46, 'first', '->', 2, 'of')\n",
      "(59, 'used', '->', 3137, 'abuse')\n",
      "(59, 'used', '->', 46, 'first')\n",
      "(156, 'against', '->', 59, 'used')\n",
      "(156, 'against', '->', 128, 'early')\n"
     ]
    }
   ],
   "source": [
    "# - num_skip: how many samples for a target word.\n",
    "\n",
    "batch, labels = generate_batch(batch_size=8, num_skips=2, skip_window=2)\n",
    "for i in range(8):\n",
    "    print(batch[i], idx2w[batch[i]],\n",
    "        '->', labels[i, 0], idx2w[labels[i, 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Construct the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "embd_size = 128  # Dimension of the embedding vector.\n",
    "skip_window = 1       # How many words to consider left and right.\n",
    "num_skips = 2         # How many times to reuse an input to generate a label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We pick a random validation set to sample nearest neighbors. Here we limit the\n",
    "# validation samples to the words that have a low numeric ID, which by\n",
    "# construction are also the most frequent.\n",
    "valid_size = 16     # Random set of words to evaluate similarity on.\n",
    "valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_neg_sampled = 64    # Number of negative examples to sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    # Input data\n",
    "    # Initilize the placeholder for the batch input (e.g. each batch\n",
    "    # could be a sentence, so the size is the number of works in the\n",
    "    # sentence)\n",
    "    train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "\n",
    "    # Initialize the placeholder for the labels, it has to be a column\n",
    "    # vector. Each word in a batch (e.g. sentence) can be a label\n",
    "    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "    \n",
    "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "    \n",
    "    with tf.device('/cpu:0'):\n",
    "        # Initialize the matrix (with random numbers) for holding embeddings. \n",
    "        # Each row is an embedding vector for a word.\n",
    "        embeddings = tf.Variable( tf.random_uniform([vocab_size, embd_size], -1.0, 1.0) )\n",
    "        \n",
    "        # Look up the vector for each of the source words in the batch.\n",
    "        # This is to associate each word with its embedding vector \n",
    "        # after estimation.\n",
    "        embd = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "        \n",
    "        # Initialize a weight matrix to hold the parameters during estimation.\n",
    "        # Each row is a coeffiient vector in a logistic regression.\n",
    "        weights = tf.Variable( tf.truncated_normal([vocab_size, embd_size], stddev=1.0/math.sqrt(embd_size)) )\n",
    "        # Initialize the bias / intercept vector. Each element is an \n",
    "        # intercept in a logistic regression.\n",
    "        biases = tf.Variable( tf.zeros([vocab_size]) )\n",
    "        \n",
    "        # Construct optimization target\n",
    "        # - num_sampled: k, i.e. number of noise words sampled from a vocabulary distribution\n",
    "        loss = tf.reduce_mean( tf.nn.nce_loss(weights, biases, embd, train_labels, num_neg_sampled, vocab_size) )\n",
    "        \n",
    "        # Construct the optimizer\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=1.0).minimize(loss)\n",
    "        \n",
    "        # Compute the cosine similarity between minibatch examples and all embeddings.\n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "        normalized_embeddings = embeddings / norm\n",
    "        valid_embeddings = tf.nn.embedding_lookup(\n",
    "          normalized_embeddings, valid_dataset)\n",
    "        similarity = tf.matmul(\n",
    "          valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "\n",
    "        # Add variable initializer.\n",
    "        init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimization target is formulated as the log-likelihood function for a logistic regresion model. Remember, our goal is to maximize the following two probabilities for a context word: \n",
    "\n",
    "$$P(\\text{a context word shows up } | \\text{ target_word})$$ \n",
    "$$P(\\text{a context word does NOT show up } | \\text{a randomly sampled noise word} )$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above model update the parameters based on one batch of the data. We will need loop it over different batches of the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_steps = 100001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "('Average loss at step ', 0, ': ', 270.61480712890625)\n",
      "Nearest to s: whispering, dogg, managers, manoeuvre, headstone, xvi, hh, fractions,\n",
      "Nearest to were: selector, constructible, dizziness, excerpts, suharto, inadvertent, auctions, charenton,\n",
      "Nearest to their: refrigerator, clerihew, commuted, unintended, citation, bust, sade, tetris,\n",
      "Nearest to state: peta, alley, microcontroller, measure, not, dignity, plausibility, wiping,\n",
      "Nearest to war: cedes, illumination, graviton, serial, starved, knighted, breadth, hemorrhage,\n",
      "Nearest to b: taupin, compensating, ominous, explained, bioses, chairperson, privileged, dessau,\n",
      "Nearest to nine: reggaeton, lfflin, lain, behavioral, pelletier, marines, whatever, grasso,\n",
      "Nearest to they: had, darkness, carla, duplicating, livy, traders, aliyah, enid,\n",
      "Nearest to years: reining, affordable, hydrazine, opaque, sundial, communion, gia, sousa,\n",
      "Nearest to called: untimely, ulrike, protesting, binary, ada, prisms, lothar, emb,\n",
      "Nearest to an: overtake, collectible, amar, marcy, burglary, mccann, ioi, barnet,\n",
      "Nearest to most: gerund, feeble, wladislaus, envoys, aviv, kilojoules, epitaph, reductionism,\n",
      "Nearest to american: nau, coordinates, kelvins, schuschnigg, hindenburg, stalls, eradicated, rightful,\n",
      "Nearest to it: barbiturates, openbsd, deposits, terribly, recluse, recall, slater, have,\n",
      "Nearest to world: sunda, adonijah, robustness, stigma, snippets, embittered, strawberry, julmust,\n",
      "Nearest to zero: rowling, diddley, anthologized, finalist, earthly, mummification, stagecoach, satisfy,\n",
      "('Average loss at step ', 2000, ': ', 114.22839235019684)\n",
      "('Average loss at step ', 4000, ': ', 52.514522559404377)\n",
      "('Average loss at step ', 6000, ': ', 33.486221997261048)\n",
      "('Average loss at step ', 8000, ': ', 23.383032896995545)\n",
      "('Average loss at step ', 10000, ': ', 17.875326181054117)\n",
      "Nearest to s: and, the, of, who, archie, vs, agave, dogg,\n",
      "Nearest to were: and, by, was, takes, rising, for, excerpts, selenium,\n",
      "Nearest to their: the, refrigerator, sounds, vs, autism, mathbf, taste, save,\n",
      "Nearest to state: measure, seems, vicinity, spassky, william, alley, zf, indian,\n",
      "Nearest to war: serial, communications, hermetical, biological, delta, austin, starved, phi,\n",
      "Nearest to b: explained, yum, succeeds, phi, domicile, contiguous, encounter, nine,\n",
      "Nearest to nine: zero, one, austin, eight, three, vs, two, phi,\n",
      "Nearest to they: had, darkness, recognised, sufferers, traders, austin, parish, vs,\n",
      "Nearest to years: zero, communion, coke, largest, df, four, regulations, elections,\n",
      "Nearest to called: ada, binary, absurd, defend, demonstrated, stampede, untimely, altenberg,\n",
      "Nearest to an: the, maximum, justice, related, christopher, vogt, tales, burglary,\n",
      "Nearest to most: mixes, peleus, necessarily, genre, astronomical, signed, supported, feeble,\n",
      "Nearest to american: schuschnigg, agent, coordinates, shows, allegedly, phi, and, strains,\n",
      "Nearest to it: he, deposits, this, ideal, archie, ho, barbiturates, suggest,\n",
      "Nearest to world: sunda, stigma, ethical, campaigns, intent, sigma, graph, resign,\n",
      "Nearest to zero: nine, eight, vs, austin, agave, one, six, archie,\n",
      "('Average loss at step ', 12000, ': ', 13.847163473367692)\n",
      "('Average loss at step ', 14000, ': ', 11.772948604464531)\n",
      "('Average loss at step ', 16000, ': ', 9.9514014550447456)\n",
      "('Average loss at step ', 18000, ': ', 8.3186723103523246)\n",
      "('Average loss at step ', 20000, ': ', 8.0253385254144671)\n",
      "Nearest to s: and, the, zero, his, circ, or, archie, of,\n",
      "Nearest to were: was, are, is, by, and, had, rising, excerpts,\n",
      "Nearest to their: the, his, hbox, sounds, circ, and, dasyprocta, s,\n",
      "Nearest to state: anastasius, psyche, jury, measure, antoninus, batll, siliceous, zf,\n",
      "Nearest to war: agouti, serial, starved, dasyprocta, delta, eug, ratify, hermetical,\n",
      "Nearest to b: d, explained, dye, agouti, phi, ankh, yum, stray,\n",
      "Nearest to nine: eight, seven, six, zero, five, agouti, three, four,\n",
      "Nearest to they: had, he, not, recognised, that, darkness, parish, sufferers,\n",
      "Nearest to years: communion, apatosaurus, four, zero, largest, coke, agouti, circ,\n",
      "Nearest to called: untimely, ada, binary, protesting, absurd, defend, demonstrated, falcon,\n",
      "Nearest to an: the, dasyprocta, justice, circ, maximum, christopher, agouti, antoninus,\n",
      "Nearest to most: aoc, agouti, feeble, mixes, peleus, necessarily, epitaph, associativity,\n",
      "Nearest to american: and, phi, schuschnigg, agent, dasyprocta, kelvins, shows, rules,\n",
      "Nearest to it: he, this, agouti, ideal, not, deposits, the, waite,\n",
      "Nearest to world: sunda, circ, incompressible, dasyprocta, agouti, intent, campaigns, stigma,\n",
      "Nearest to zero: eight, nine, five, six, seven, three, circ, agouti,\n",
      "('Average loss at step ', 22000, ': ', 6.8230736273527146)\n",
      "('Average loss at step ', 24000, ': ', 6.7924152430295948)\n",
      "('Average loss at step ', 26000, ': ', 6.6655439414978028)\n",
      "('Average loss at step ', 28000, ': ', 6.3414805072546008)\n",
      "('Average loss at step ', 30000, ': ', 5.9023096637725834)\n",
      "Nearest to s: and, the, his, zero, two, circ, agouti, or,\n",
      "Nearest to were: are, was, is, by, had, rising, be, excerpts,\n",
      "Nearest to their: his, the, its, s, hbox, out, dasyprocta, circ,\n",
      "Nearest to state: anastasius, psyche, jury, measure, antoninus, zf, circ, siliceous,\n",
      "Nearest to war: agouti, serial, starved, eug, hermetical, ratify, illumination, dasyprocta,\n",
      "Nearest to b: d, and, explained, dye, agouti, one, phi, stray,\n",
      "Nearest to nine: eight, seven, six, five, four, zero, three, agouti,\n",
      "Nearest to they: he, we, not, it, recognised, parish, had, deva,\n",
      "Nearest to years: communion, reining, tnf, four, six, apatosaurus, largest, coke,\n",
      "Nearest to called: protesting, untimely, ada, absurd, binary, falcon, and, defend,\n",
      "Nearest to an: the, justice, dek, dasyprocta, circ, antoninus, agouti, noticeable,\n",
      "Nearest to most: aoc, feeble, agouti, mixes, peleus, gerund, contend, associativity,\n",
      "Nearest to american: and, phi, agent, schuschnigg, aldosterone, dasyprocta, one, shows,\n",
      "Nearest to it: he, this, agouti, which, there, also, ideal, not,\n",
      "Nearest to world: sunda, intent, incompressible, circ, ifvs, dasyprocta, adonijah, agouti,\n",
      "Nearest to zero: five, eight, six, nine, seven, four, circ, three,\n",
      "('Average loss at step ', 32000, ': ', 5.9643589117527007)\n",
      "('Average loss at step ', 34000, ': ', 5.6986327357292179)\n",
      "('Average loss at step ', 36000, ': ', 5.7752253620624545)\n",
      "('Average loss at step ', 38000, ': ', 5.5368903442621233)\n",
      "('Average loss at step ', 40000, ': ', 5.2804120640754704)\n",
      "Nearest to s: his, and, zero, agouti, or, circ, two, hbox,\n",
      "Nearest to were: are, was, had, is, have, be, by, been,\n",
      "Nearest to their: his, the, its, historiae, clocking, hbox, dasyprocta, out,\n",
      "Nearest to state: psyche, anastasius, measure, peta, jury, circ, zf, antoninus,\n",
      "Nearest to war: agouti, serial, eug, starved, hermetical, ratify, ks, illumination,\n",
      "Nearest to b: d, explained, eight, UNKOWN, six, agouti, zero, dye,\n",
      "Nearest to nine: eight, zero, seven, six, five, four, three, agouti,\n",
      "Nearest to they: he, we, it, not, you, there, parish, recognised,\n",
      "Nearest to years: four, communion, flatted, six, apatosaurus, tnf, reining, coke,\n",
      "Nearest to called: protesting, flores, UNKOWN, absurd, untimely, and, ada, prisms,\n",
      "Nearest to an: the, justice, dek, dasyprocta, answer, circ, burglary, antoninus,\n",
      "Nearest to most: aoc, agouti, feeble, mixes, gerund, peleus, some, contend,\n",
      "Nearest to american: and, agent, phi, aldosterone, schuschnigg, kelvins, vma, dasyprocta,\n",
      "Nearest to it: he, this, which, there, agouti, they, that, not,\n",
      "Nearest to world: sunda, intent, stigma, renounce, circ, graph, campaigns, incompressible,\n",
      "Nearest to zero: eight, seven, six, five, four, nine, agouti, three,\n",
      "('Average loss at step ', 42000, ': ', 5.3613444348573687)\n",
      "('Average loss at step ', 44000, ': ', 5.2003517194986344)\n",
      "('Average loss at step ', 46000, ': ', 5.2579990692138674)\n",
      "('Average loss at step ', 48000, ': ', 5.2432154005765916)\n",
      "('Average loss at step ', 50000, ': ', 4.9933790647983551)\n",
      "Nearest to s: his, and, zero, the, or, of, circ, agouti,\n",
      "Nearest to were: are, was, had, have, is, be, rising, been,\n",
      "Nearest to their: his, its, the, her, historiae, clocking, hbox, dasyprocta,\n",
      "Nearest to state: psyche, anastasius, jury, peta, zf, pds, wiping, measure,\n",
      "Nearest to war: agouti, serial, eug, starved, hermetical, ks, ratify, dasyprocta,\n",
      "Nearest to b: d, explained, six, agouti, dye, frogs, phi, f,\n",
      "Nearest to nine: eight, six, seven, zero, five, three, agouti, four,\n",
      "Nearest to they: he, we, it, there, not, you, who, parish,\n",
      "Nearest to years: communion, flatted, tnf, apatosaurus, reining, four, coke, retention,\n",
      "Nearest to called: protesting, and, flores, absurd, falcon, untimely, UNKOWN, ada,\n",
      "Nearest to an: justice, the, dek, spreading, burglary, antoninus, answer, dasyprocta,\n",
      "Nearest to most: aoc, feeble, some, agouti, gerund, contend, more, mixes,\n",
      "Nearest to american: and, phi, dasyprocta, vma, kelvins, agent, aldosterone, schuschnigg,\n",
      "Nearest to it: he, this, which, there, agouti, circ, they, archaeopteryx,\n",
      "Nearest to world: sunda, two, intent, graph, circ, stacey, dasyprocta, stigma,\n",
      "Nearest to zero: eight, five, six, seven, four, nine, three, circ,\n",
      "('Average loss at step ', 52000, ': ', 5.0248908026814458)\n",
      "('Average loss at step ', 54000, ': ', 5.1689684797525404)\n",
      "('Average loss at step ', 56000, ': ', 5.0432325884103779)\n",
      "('Average loss at step ', 58000, ': ', 5.0581360259056094)\n",
      "('Average loss at step ', 60000, ': ', 4.9500770815610888)\n",
      "Nearest to s: and, his, zero, agouti, circ, thibetanus, ursus, vs,\n",
      "Nearest to were: are, was, had, have, rising, be, been, is,\n",
      "Nearest to their: his, its, the, her, historiae, some, dasyprocta, clocking,\n",
      "Nearest to state: psyche, anastasius, zf, circ, peta, pds, jury, wiping,\n",
      "Nearest to war: agouti, serial, eug, starved, ratify, ks, hermetical, scuba,\n",
      "Nearest to b: d, explained, ursus, UNKOWN, f, frogs, six, pulau,\n",
      "Nearest to nine: eight, six, seven, five, four, zero, agouti, three,\n",
      "Nearest to they: he, we, there, it, you, not, who, parish,\n",
      "Nearest to years: flatted, communion, apatosaurus, reining, tnf, coke, four, retention,\n",
      "Nearest to called: protesting, flores, absurd, falcon, prisms, untimely, ada, timberlake,\n",
      "Nearest to an: justice, burglary, spreading, dek, electrode, antoninus, noticeable, answer,\n",
      "Nearest to most: aoc, some, feeble, more, agouti, contend, mixes, gerund,\n",
      "Nearest to american: and, kelvins, british, dasyprocta, agent, aldosterone, phi, schuschnigg,\n",
      "Nearest to it: he, this, there, which, agouti, they, circ, she,\n",
      "Nearest to world: sunda, graph, intent, stacey, circ, stigma, dasyprocta, agouti,\n",
      "Nearest to zero: eight, seven, six, five, nine, four, circ, three,\n",
      "('Average loss at step ', 62000, ': ', 4.9932150194644924)\n",
      "('Average loss at step ', 64000, ': ', 4.8488163149952888)\n",
      "('Average loss at step ', 66000, ': ', 4.6136231681108475)\n",
      "('Average loss at step ', 68000, ': ', 4.967822315692902)\n",
      "('Average loss at step ', 70000, ': ', 4.9035535144805911)\n",
      "Nearest to s: his, and, callithrix, thibetanus, acm, agouti, fundamentals, circ,\n",
      "Nearest to were: are, was, had, have, been, be, rising, selenium,\n",
      "Nearest to their: his, its, the, her, some, historiae, thaler, them,\n",
      "Nearest to state: psyche, callithrix, zf, spassky, anastasius, peta, circ, pds,\n",
      "Nearest to war: agouti, serial, ratify, eug, starved, hermetical, ks, scuba,\n",
      "Nearest to b: d, UNKOWN, ursus, microcebus, explained, f, seven, six,\n",
      "Nearest to nine: eight, seven, six, five, four, zero, three, agouti,\n",
      "Nearest to they: he, we, there, you, it, not, who, glasnost,\n",
      "Nearest to years: flatted, communion, apatosaurus, reining, tnf, cebus, four, five,\n",
      "Nearest to called: protesting, flores, UNKOWN, absurd, falcon, untimely, visuals, honestly,\n",
      "Nearest to an: justice, spreading, burglary, the, callithrix, answer, antoninus, dek,\n",
      "Nearest to most: some, aoc, more, feeble, agouti, nonnegative, contend, solicitation,\n",
      "Nearest to american: callithrix, kelvins, and, british, agent, schuschnigg, hindenburg, aldosterone,\n",
      "Nearest to it: he, this, there, which, she, they, agouti, microcebus,\n",
      "Nearest to world: sunda, stacey, intent, graph, stigma, circ, dasyprocta, ifvs,\n",
      "Nearest to zero: eight, five, six, seven, four, nine, three, circ,\n",
      "('Average loss at step ', 72000, ': ', 4.7518867194652561)\n",
      "('Average loss at step ', 74000, ': ', 4.8075688235759735)\n",
      "('Average loss at step ', 76000, ': ', 4.7170394104719158)\n",
      "('Average loss at step ', 78000, ': ', 4.7977750127911571)\n",
      "('Average loss at step ', 80000, ': ', 4.807642081618309)\n",
      "Nearest to s: his, zero, callithrix, agouti, mishnayot, circ, thibetanus, fundamentals,\n",
      "Nearest to were: are, was, had, have, been, be, rising, selenium,\n",
      "Nearest to their: its, his, the, her, some, thaler, historiae, clocking,\n",
      "Nearest to state: psyche, peta, callithrix, zf, pds, spassky, ursus, circ,\n",
      "Nearest to war: agouti, serial, ratify, eug, hermetical, starved, scuba, evidences,\n",
      "Nearest to b: d, UNKOWN, ursus, explained, microcebus, f, agouti, pulau,\n",
      "Nearest to nine: eight, seven, six, five, four, zero, agouti, three,\n",
      "Nearest to they: he, we, there, you, it, who, not, glasnost,\n",
      "Nearest to years: flatted, apatosaurus, communion, tnf, cebus, reining, retention, multilinear,\n",
      "Nearest to called: protesting, flores, pontificia, absurd, falcon, termed, honestly, prisms,\n",
      "Nearest to an: justice, burglary, astrobiology, spreading, answer, callithrix, dek, colonists,\n",
      "Nearest to most: some, more, aoc, many, agouti, contend, solicitation, pelts,\n",
      "Nearest to american: tektites, british, callithrix, kelvins, music, agent, dasyprocta, announcers,\n",
      "Nearest to it: he, this, there, which, she, they, agouti, circ,\n",
      "Nearest to world: sunda, stacey, intent, graph, stigma, circ, dasyprocta, kalamazoo,\n",
      "Nearest to zero: five, seven, six, four, eight, nine, circ, agouti,\n",
      "('Average loss at step ', 82000, ': ', 4.7734601597785948)\n",
      "('Average loss at step ', 84000, ': ', 4.7476883018016816)\n",
      "('Average loss at step ', 86000, ': ', 4.7736041860580443)\n",
      "('Average loss at step ', 88000, ': ', 4.74708021903038)\n",
      "('Average loss at step ', 90000, ': ', 4.712145458340645)\n",
      "Nearest to s: his, and, callithrix, circ, agouti, zero, mishnayot, arexx,\n",
      "Nearest to were: are, was, had, have, be, been, rising, selenium,\n",
      "Nearest to their: its, his, the, her, some, historiae, them, thaler,\n",
      "Nearest to state: psyche, callithrix, peta, zf, spassky, pds, ursus, agouti,\n",
      "Nearest to war: agouti, ratify, eug, serial, hermetical, scuba, starved, manure,\n",
      "Nearest to b: d, UNKOWN, explained, f, ursus, microcebus, six, r,\n",
      "Nearest to nine: eight, seven, six, five, four, zero, three, agouti,\n",
      "Nearest to they: he, we, there, you, not, it, who, these,\n",
      "Nearest to years: flatted, apatosaurus, communion, tnf, cebus, reining, multilinear, glendale,\n",
      "Nearest to called: protesting, flores, absurd, pontificia, altenberg, honestly, falcon, termed,\n",
      "Nearest to an: burglary, justice, spreading, astrobiology, colonists, electrode, dek, any,\n",
      "Nearest to most: some, more, many, aoc, agouti, pelts, solicitation, contend,\n",
      "Nearest to american: tektites, british, callithrix, kelvins, music, vdc, announcers, schuschnigg,\n",
      "Nearest to it: he, this, there, which, she, they, agouti, circ,\n",
      "Nearest to world: peacocks, sunda, stacey, graph, intent, stigma, khartoum, circ,\n",
      "Nearest to zero: five, eight, six, seven, nine, four, circ, three,\n",
      "('Average loss at step ', 92000, ': ', 4.6639277170896527)\n",
      "('Average loss at step ', 94000, ': ', 4.7329651384353637)\n",
      "('Average loss at step ', 96000, ': ', 4.6850878338813784)\n",
      "('Average loss at step ', 98000, ': ', 4.608861551940441)\n",
      "('Average loss at step ', 100000, ': ', 4.6936832878589634)\n",
      "Nearest to s: his, callithrix, agouti, and, circ, mishnayot, thibetanus, mico,\n",
      "Nearest to were: are, was, had, have, be, been, is, those,\n",
      "Nearest to their: its, his, her, the, some, thaler, them, mico,\n",
      "Nearest to state: peta, psyche, callithrix, zf, spassky, pds, ursus, agouti,\n",
      "Nearest to war: agouti, ratify, eug, hermetical, during, serial, scuba, starved,\n",
      "Nearest to b: d, f, ursus, microcebus, explained, UNKOWN, pulau, r,\n",
      "Nearest to nine: eight, seven, six, five, zero, four, three, agouti,\n",
      "Nearest to they: he, there, we, you, it, not, glasnost, she,\n",
      "Nearest to years: flatted, communion, apatosaurus, glendale, tnf, milligan, multilinear, twists,\n",
      "Nearest to called: protesting, honestly, and, termed, flores, absurd, used, falcon,\n",
      "Nearest to an: justice, burglary, astrobiology, spreading, colonists, electrode, dek, antoninus,\n",
      "Nearest to most: some, more, many, aoc, agouti, solicitation, nonnegative, dasyprocta,\n",
      "Nearest to american: british, tektites, and, callithrix, vdc, kelvins, german, schuschnigg,\n",
      "Nearest to it: he, this, there, she, which, they, agouti, circ,\n",
      "Nearest to world: peacocks, sunda, stacey, graph, intent, stigma, circ, dasyprocta,\n",
      "Nearest to zero: eight, five, seven, four, nine, six, circ, three,\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as session:\n",
    "    # We must initialize all variables before we use them.\n",
    "    init.run()\n",
    "    print(\"Initialized\")\n",
    "    \n",
    "    avg_loss = 0\n",
    "    for step in xrange(num_steps):\n",
    "        # Generate a new batch of data\n",
    "        batch_inputs, batch_labels = generate_batch(batch_size, num_skips, skip_window)\n",
    "        feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}\n",
    "        \n",
    "        _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "        avg_loss += loss_val\n",
    "        \n",
    "        if step % 2000 == 0:\n",
    "            if step > 0:\n",
    "                avg_loss /= 2000\n",
    "            # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "            print(\"Average loss at step \", step, \": \", avg_loss)\n",
    "            avg_loss = 0\n",
    "\n",
    "        # Note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "        if step % 10000 == 0:\n",
    "          sim = similarity.eval()\n",
    "          for i in xrange(valid_size):\n",
    "            valid_word = idx2w[valid_examples[i]]\n",
    "            top_k = 8  # number of nearest neighbors\n",
    "            nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
    "            log_str = \"Nearest to %s:\" % valid_word\n",
    "            for k in xrange(top_k):\n",
    "              close_word = idx2w[nearest[k]]\n",
    "              log_str = \"%s %s,\" % (log_str, close_word)\n",
    "            print(log_str)\n",
    "            \n",
    "    final_embeddings = normalized_embeddings.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
