{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Example: Multi-input with 10 Classes\n",
    "\n",
    "January 29, 2017\n",
    "\n",
    "Pan Chao\n",
    "\n",
    "\n",
    "This is a dummy example to show how to use a NN with multiple input and multiple classes. We will do:\n",
    "\n",
    "- Condfigure a model with Merge layer\n",
    "- Simulate data\n",
    "- Train the model\n",
    "\n",
    "\n",
    "# Questions\n",
    "\n",
    "- What is the motivation for NN with multiple input?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "left_branch = Sequential()\n",
    "left_branch.add(Dense(32, input_dim=784))\n",
    "\n",
    "right_branch = Sequential()\n",
    "right_branch.add(Dense(32, input_dim=784))\n",
    "\n",
    "merged = Merge([left_branch, right_branch], mode='concat')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(merged)\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above model configuration, the optimization used is **Root Mean Square Propagation (RMSprop)**. The following introduction is from https://devblogs.nvidia.com/parallelforall/deep-learning-nutshell-history-training/ :\n",
    "\n",
    "    RMSprop keeps track of the weighted running mean of the squared gradient and then divides each calculated gradient by the square root of this weighted running mean (it essentially normalizes the gradient by dividing by the magnitude of recent gradients). The consequence is that when a plateau in the error surface is encountered and the gradient is very small, the updates take greater steps, ensuring faster learning (a small update: 0.00001, the square root of the weighted average: 0.00005, update size: 0.2). On the other hand, RMSprop protects against exploding gradients (a large update: 100, the square root of the weighted average: 25, update size: 4) and is thus used frequently in recurrent neural networks and LSTMs to protect both against vanishing and exploding gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Simulate data\n",
    "\n",
    "import numpy as np\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "data_1 = np.random.random((1000, 784))\n",
    "data_2 = np.random.random((1000, 784))\n",
    "\n",
    "labels = np.random.randint(10, size=(1000, 1))\n",
    "labels = to_categorical(labels, 10)  # i.e. convert to 1-of-K coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 0s - loss: 2.9996 - acc: 0.0890     \n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 0s - loss: 2.5694 - acc: 0.1010     \n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 0s - loss: 2.4985 - acc: 0.1350     \n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 0s - loss: 2.3329 - acc: 0.1750     \n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 0s - loss: 2.2522 - acc: 0.1760     \n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 0s - loss: 2.2017 - acc: 0.2000     \n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 0s - loss: 2.0409 - acc: 0.2590     \n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 0s - loss: 1.9807 - acc: 0.2990     \n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 0s - loss: 1.9002 - acc: 0.3550     \n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 0s - loss: 1.8347 - acc: 0.3720     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11b99cf50>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "\n",
    "model.fit([data_1, data_2], labels, nb_epoch=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
